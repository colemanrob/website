[{"authors":["admin"],"categories":null,"content":"I make notes about how to solve problems using r, an open-source programming language for data science. Also interested in active labour market policy, search and matching theory, and jobs data.\nI live in the northern part of Ontario, Canada with this furry brown monster. I write these notes to learn out loud, keep a second searchable brain, and improve my thinking/writing. ðŸ“š â˜• ðŸš² ðŸ’»\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/rob-coleman/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rob-coleman/","section":"authors","summary":"I make notes about how to solve problems using r, an open-source programming language for data science. Also interested in active labour market policy, search and matching theory, and jobs data.","tags":null,"title":"Rob Coleman","type":"authors"},{"authors":null,"categories":null,"content":"Following along with Richard McElreaths Statistical Rethinking\n  textbook  youtube lectures  tidyversion  ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"ba25b3d6690d184c0d6d3d592f39a131","permalink":"/courses/stat_rethink/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/stat_rethink/","section":"courses","summary":"Statistical Rethinking by Richard McElreath -- A Bayesian Course in R","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"Philosophy of Science  McElreath offers good insights on the philosophy of science and calls for statistical rethinking statistical models are like the golum of prague, unthinking instruction followers.  bad/poor instructions will destroy Prague bad/poor models will compute but not tell us anything     Bayesian Toolkit  instead we need a broader toolkit to build models and understand the causes:  Bayesian data analysis \u0026ndash; counting the number of ways the data could have happened given our assumptions Model comparison \u0026ndash; using cross-validation and information criteria to compare models predictions Multi-level models \u0026ndash; accounting for unmeasured aspects of individuals, groups, or populations. Graphical causal models \u0026ndash; designing models for the purposes of identifying causes    ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"342e402c6744e67fd81b98e23f84c077","permalink":"/courses/stat_rethink/chapter1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/stat_rethink/chapter1/","section":"courses","summary":"Philosophy of Science  McElreath offers good insights on the philosophy of science and calls for statistical rethinking statistical models are like the golum of prague, unthinking instruction followers.  bad/poor instructions will destroy Prague bad/poor models will compute but not tell us anything     Bayesian Toolkit  instead we need a broader toolkit to build models and understand the causes:  Bayesian data analysis \u0026ndash; counting the number of ways the data could have happened given our assumptions Model comparison \u0026ndash; using cross-validation and information criteria to compare models predictions Multi-level models \u0026ndash; accounting for unmeasured aspects of individuals, groups, or populations.","tags":null,"title":"Chapter 1","type":"docs"},{"authors":null,"categories":null,"content":"Building up Bayesian Inference  small world (the model) vs large world (reality) - goal of science is to help us move between bayesian inference is counting and comparing possibilities, the garden of forking data globe tossing example. Variables:  9 tosses - n = 9 W water x 6 L land x 3 p proportion of globe covered by water - parameters (unobserved variables)     Likelihood  the number of ways the data are possible after eliminating the ways that are inconsistent with out data (i.e. counting the garden)  The relative number of ways of getting 6 water, holding p at 0.5, and N = L+W at 9.\ndbinom(6, size = 9, prob = 0.5)  ## [1] 0.1640625  Prior p (the probability of sampling water) is the parameter of the model. For every parameter in your model you must provide a distribution of prior plausibility. Where do priors come from? Engineering choice, domain knowledge etc.\nPosterior Update the prior given the data - for every unique combination of data, likelihood, parameters and prior there is a unique posterior distribution \u0026ndash; the relative plausibility of different parameter values conditional on the data.\nBayesian Updating\n3 different conditioning engines or techniques for computing posterior distributions:\n grid approximation quadratic approximation markov chain monte carlo (MCMC)  Grid Approximation Constrain the possible parameter values to only a finite grid of values.\n# define grid p_grid \u0026lt;- seq(from = 0, to = 1, length.out = 20) # define prior prior \u0026lt;- rep(1, 20) # compute likelihood at each value in grid likelihood \u0026lt;- dbinom(6, size = 9, prob = p_grid) # compute the product of likelihood and prior unstd_posterior \u0026lt;- likelihood * prior # standardize the posterior so it sums to 1 posterior \u0026lt;- unstd_posterior / sum(unstd_posterior)  plot the results\nplot(p_grid, posterior, type = \u0026quot;b\u0026quot;, xlab = \u0026quot;probability of water\u0026quot;, ylab = \u0026quot;posterior probability\u0026quot;) mtext(\u0026quot;20 points\u0026quot;)  Quadratic Approximation Find the peak/curve of the distribution\nuse the quap function from the rethinking package\nlibrary(rethinking) # specify globe_qa \u0026lt;- quap( alist( W ~ dbinom(W+L, p), #binomial likelihood p ~ dunif(0,1) # uniform prior ), data = list(W=6, L=3) ) # display the summary precis(globe_qa)  ## mean sd 5.5% 94.5% ## p 0.6666665 0.1571338 0.4155363 0.9177967  MCMC Instead of computing or approximating the posterior distribution, MCMC draws samples from the posterior - you end up with samples whose frequencies correspond to posterior plausibilities.\nn_samples \u0026lt;- 1000 p \u0026lt;- rep(NA, n_samples) p[1] \u0026lt;- 0.5 W \u0026lt;- 6 L \u0026lt;- 3 for (i in 2:n_samples) { p_new \u0026lt;- rnorm(1, p[i-1], 0.1) if (p_new \u0026lt; 0 ) p_new \u0026lt;- abs(p_new) if (p_new \u0026gt; 1 ) p_new \u0026lt;- 2 - p_new q0 \u0026lt;- dbinom(W, W+L, p[i-1]) q1 \u0026lt;- dbinom(W, W+L, p_new) p[i] \u0026lt;- ifelse(runif(1) \u0026lt; q1/q0, p_new, p[i-1]) } dens(p, xlim=c(0,1)) curve(dbeta(x, W+1, L+1), lty = 2, add=TRUE)  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"fc476ac58e9ac229ecf3024bdc1a74cb","permalink":"/courses/stat_rethink/chapter2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/stat_rethink/chapter2/","section":"courses","summary":"Building up Bayesian Inference  small world (the model) vs large world (reality) - goal of science is to help us move between bayesian inference is counting and comparing possibilities, the garden of forking data globe tossing example.","tags":null,"title":"Chapter 2","type":"docs"},{"authors":null,"categories":null,"content":"Sampling from the imaginary Now let\u0026rsquo;s sample from the posterior\np_grid \u0026lt;- seq( from=0, to=1, length.out = 1000) prob_p \u0026lt;- rep(1, 1000) prob_data \u0026lt;- dbinom(6, size=9, prob= p_grid) posterior \u0026lt;- prob_data * prob_p posterior \u0026lt;- posterior / sum(posterior)  samples \u0026lt;- sample(p_grid, prob = posterior, size=1e4, replace = TRUE)  plot(samples)  library(rethinking) dens(samples)  Intervals of defined boundaries # add up posterior prob where p \u0026lt; 0.5 sum(posterior[p_grid \u0026lt; 0.5])  ## [1] 0.1718746  quantile(samples, 0.8)  ## 80% ## 0.7607608  quantile(samples, c(0.1, 0.9))  ## 10% 90% ## 0.4473473 0.8128128  PI(samples, prob = 0.5)  ## 25% 75% ## 0.5415415 0.7387387  Sampling to simulate prediction dbinom(0:2, size=2, prob=0.7)  ## [1] 0.09 0.42 0.49  rbinom(10, size = 2, prob = 0.7)  ## [1] 2 1 2 2 2 1 0 2 2 2  dummy_w \u0026lt;- rbinom(1e5, size = 9, prob = 0.7) simplehist(dummy_w, xlab=\u0026quot;dummy water count\u0026quot;)  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"7e95a0f06eda9a0d1fabac102e32a8c9","permalink":"/courses/stat_rethink/chapter3/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/stat_rethink/chapter3/","section":"courses","summary":"Sampling from the imaginary Now let\u0026rsquo;s sample from the posterior\np_grid \u0026lt;- seq( from=0, to=1, length.out = 1000) prob_p \u0026lt;- rep(1, 1000) prob_data \u0026lt;- dbinom(6, size=9, prob= p_grid) posterior \u0026lt;- prob_data * prob_p posterior \u0026lt;- posterior / sum(posterior)  samples \u0026lt;- sample(p_grid, prob = posterior, size=1e4, replace = TRUE)  plot(samples)  library(rethinking) dens(samples)  Intervals of defined boundaries # add up posterior prob where p \u0026lt; 0.","tags":null,"title":"Chapter 3","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":"Never noticed type_convert from readr that lets you wrangle data and then convert as appropriate. Let\u0026rsquo;s give it a spin.\nlibrary(tidyverse) typical_data \u0026lt;- tribble(~bad_col, \u0026quot;3_apple\u0026quot;, \u0026quot;4_banana\u0026quot;) typical_data %\u0026gt;% separate(bad_col, into = c(\u0026quot;n\u0026quot;, \u0026quot;fruit\u0026quot;), sep = \u0026quot;_\u0026quot;) %\u0026gt;% type_convert()  ## # A tibble: 2 x 2 ## n fruit ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 3 apple ## 2 4 banana  Perfect!\n","date":1590883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590883200,"objectID":"21a842d16d7922b97f57554dabdafd2f","permalink":"/code/type-convert/","publishdate":"2020-05-31T00:00:00Z","relpermalink":"/code/type-convert/","section":"code","summary":"Never noticed type_convert from readr that lets you wrangle data and then convert as appropriate. Let\u0026rsquo;s give it a spin.\nlibrary(tidyverse) typical_data \u0026lt;- tribble(~bad_col, \u0026quot;3_apple\u0026quot;, \u0026quot;4_banana\u0026quot;) typical_data %\u0026gt;% separate(bad_col, into = c(\u0026quot;n\u0026quot;, \u0026quot;fruit\u0026quot;), sep = \u0026quot;_\u0026quot;) %\u0026gt;% type_convert()  ## # A tibble: 2 x 2 ## n fruit ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 3 apple ## 2 4 banana  Perfect!","tags":["Tidyverse"],"title":"Type Convert","type":"code"},{"authors":[],"categories":[],"content":"Recently had the pleasure of fuzzy matching two datasets that included first and last name and date of birth. Here are some notes:\n  fuzzyjoin package - great functions that do most of the heavy lifting  string metrics - there are a lot of ways to measure the difference in strings, selecting a metric depends on the problem  Toy Example library(tidyverse) library(fuzzyjoin)  Create our list of names and dobs.\ninside_list \u0026lt;- tribble( ~first, ~last, ~dob, \u0026quot;frank\u0026quot;, \u0026quot;johnson\u0026quot;, \u0026quot;1980-01-01\u0026quot;, \u0026quot;debbie\u0026quot;, \u0026quot;willis\u0026quot;, \u0026quot;1982-01-01\u0026quot; ) %\u0026gt;% mutate(dob = parse_date(dob, format = \u0026quot;%Y-%m-%d\u0026quot;))  Create a second list to be matched against\noutside_list \u0026lt;- tribble( ~first, ~last, ~dob, \u0026quot;frank\u0026quot;, \u0026quot;johnson\u0026quot;, \u0026quot;1980-01-01\u0026quot;, \u0026quot;debrah\u0026quot;, \u0026quot;willis\u0026quot;, \u0026quot;1982-01-02\u0026quot; ) %\u0026gt;% mutate(dob = parse_date(dob, format = \u0026quot;%Y-%m-%d\u0026quot;))  Use the function of your choice.\nstringdist_join(outside_list, inside_list, by = c(\u0026quot;first\u0026quot;, \u0026quot;last\u0026quot;, \u0026quot;dob\u0026quot;), ignore_case = TRUE, distance_col = \u0026quot;dist\u0026quot;, mode = \u0026quot;left\u0026quot;, method = \u0026quot;lv\u0026quot;, max_dist = 20) %\u0026gt;% mutate(total_distance = dob.dist + first.dist + last.dist) %\u0026gt;% select(-ends_with(\u0026quot;.dist\u0026quot;)) %\u0026gt;% knitr::kable()     first.x last.x dob.x first.y last.y dob.y dist total_distance     frank johnson 1980-01-01 frank johnson 1980-01-01 NA 0   frank johnson 1980-01-01 debbie willis 1982-01-01 NA 14   debrah willis 1982-01-02 frank johnson 1980-01-01 NA 14   debrah willis 1982-01-02 debbie willis 1982-01-01 NA 4    You can see the difference between debrah and debbie is 4.\nSetting the distance threshold is often a function of how exact you want the matches to be.\nSee the fuzzyjoin reference for more.\n","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570233600,"objectID":"de0e982312f2878562efc191e6de6251","permalink":"/code/notes-on-fuzzy-matching/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/code/notes-on-fuzzy-matching/","section":"code","summary":"Recently had the pleasure of fuzzy matching two datasets that included first and last name and date of birth. Here are some notes:\n  fuzzyjoin package - great functions that do most of the heavy lifting  string metrics - there are a lot of ways to measure the difference in strings, selecting a metric depends on the problem  Toy Example library(tidyverse) library(fuzzyjoin)  Create our list of names and dobs.","tags":["Other"],"title":"Notes on fuzzy matching","type":"code"},{"authors":[],"categories":[],"content":"Rodrigo Fernandezi, Herwig Immervolli, Daniele Pacificoi and CÃ©line ThÃ©venoti, 2016\n  this research advances new methods to categorize employment barriers among jobseekers so as to design appropriately tailored activation responses across policy and institutional domains. little is known about specific employment barriers at the individual level, instead broad proxies are used to describe populations (NEET, older workers etc) instead the authors try a bottom up approach by categorizing and measuring employment barriers using latest class analysis (LDA), a clustering method, the authors find more useful groups of jobseekers who share common employment barriers by analyzing the European Union Statisitcs on Income and Living Conditions (EU SILC) available here  Types of Employment Barriers  lack of work-relateed capabilities  a range of factors that may limit an individuals\u0026rsquo; capacity to perform a specific task (e.g. lack of educations, skills or work experience, care responsibilities, or health limitations)   lack of work incentives  weak incentives to look for or accept a good job - (e.g. low pay, generous out of work benefits)   lack of employment opportunities (regional)  a small number of vacancies in the labour market segment due to information asymmetries, skills mismatches, discrimination    ","date":1564876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564936761,"objectID":"a8be9d1f04ff4e12ff6b642e440aadcd","permalink":"/notes/notes-on-employment-barriers/","publishdate":"2019-08-04T00:00:00Z","relpermalink":"/notes/notes-on-employment-barriers/","section":"notes","summary":"Rodrigo Fernandezi, Herwig Immervolli, Daniele Pacificoi and CÃ©line ThÃ©venoti, 2016\n  this research advances new methods to categorize employment barriers among jobseekers so as to design appropriately tailored activation responses across policy and institutional domains.","tags":[],"title":"Notes on Employment Barriers","type":"notes"},{"authors":["Rob Coleman"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"I was inspired by David Robinson\u0026lsquo;s latest webcast in which he made a heatmap of French train delays using geom_tile and wanted to try it out for myself.\n In this week\u0026#39;s #tidytuesday screencast, I analyze delays in French train stations ðŸ‡«ðŸ‡·ðŸš„\nI show how to create heatmaps of delays (inspired by @noccaea!), and embarrass myself with even the simplest French pronunciationshttps://t.co/zBrkIkdcCz #rstats pic.twitter.com/RI7ZpxV89X\n\u0026mdash; David Robinson (@drob) February 26, 2019   I don\u0026rsquo;t have a lot of opportunities to use heatmaps, but recently Statcan has released their Journey to Work data as part of the 2016 Census. I wanted to see if I could use a heatmap to understand the commuting patterns in communities in southern Ontario.\nLoad packages library(tidyverse) library(cancensus)  I downloaded the commuting table from statscan because I couldn\u0026rsquo;t find it using the cancensus package.\nraw_commute \u0026lt;- read_csv(\u0026quot;~/projects/R stuff/commute/98-400-X2016391_English_CSV_data.csv\u0026quot;) %\u0026gt;% janitor::clean_names() %\u0026gt;% select(code = geo_code_por, live = geo_name, work = geo_name_1, total = dim_sex_3_member_id_1_total_sex)  # filter only those whose code starts w/ 35 (Ontario) ontario_commute \u0026lt;- raw_commute %\u0026gt;% filter(str_detect(code, pattern = \u0026quot;^35\u0026quot;))  Download geography data I was able to retrieve the working age population and census division geographies using cancensus\nlibrary(cancensus) library(sf)  ## Linking to GEOS 3.7.0, GDAL 2.3.2, PROJ 5.2.0  # create list of ontario census divisions to pass to cancensus regions_list_ontario \u0026lt;- list_census_regions(\u0026quot;CA16\u0026quot;) %\u0026gt;% filter(str_detect(region, pattern = \u0026quot;^35\u0026quot;)) %\u0026gt;% as_census_region_list  ## Querying CensusMapper API for regions data...  pop_data \u0026lt;- get_census(\u0026quot;CA16\u0026quot;, regions = regions_list_ontario, vectors = \u0026quot;v_CA16_61\u0026quot;, level = \u0026quot;CD\u0026quot;, geo_format = \u0026quot;sf\u0026quot;, labels = \u0026quot;short\u0026quot;) %\u0026gt;% janitor::clean_names()  # clean pop_data %\u0026gt;% mutate(working_age = v_ca16_61) %\u0026gt;% mutate(code = as.double(geo_uid)) %\u0026gt;% select(code, working_age, shape_area, geometry) -\u0026gt; pop_data_clean  Joining the tables together # remove commuting within cd, compute totals ontario_commute %\u0026gt;% filter(live != work) %\u0026gt;% group_by(live) %\u0026gt;% mutate(total_commuters = sum(total), prop_commuters = total / total_commuters ) %\u0026gt;% ungroup() %\u0026gt;% left_join(pop_data_clean, by = \u0026quot;code\u0026quot;) %\u0026gt;% group_by(work) %\u0026gt;% mutate(total_commuters_destination = sum(total)) %\u0026gt;% mutate(live_prop = total_commuters / working_age, work_prop = total / working_age ) %\u0026gt;% ungroup() -\u0026gt; ontario_commute_clean  Create visualizations Let\u0026rsquo;s see what visualizations will work.\nontario_commute_clean %\u0026gt;% filter(total \u0026gt;= 3000) %\u0026gt;% mutate(live = str_wrap(live, width = 15)) %\u0026gt;% mutate(live = fct_reorder(live, total)) %\u0026gt;% mutate(work = fct_reorder(work, total)) %\u0026gt;% ggplot(aes(live, work, fill = total)) + geom_tile(alpha = 0.7) + theme_light() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + scale_fill_viridis_c(labels = scales::comma_format()) + labs(fill = \u0026quot;# of commuters\u0026quot;, title = \u0026quot;The largest number of commuters in Ontario are those who live in York, Peel and Durham who commute to Toronto for work\u0026quot;, subtitle = \u0026quot;Number of employed labour force aged 15+ who commute by Census Division - Minimum 3,000 commuters to be represented\u0026quot;, x = \u0026quot;Live - Census Division\u0026quot;, y = \u0026quot;Work - Census Division\u0026quot;, caption = \u0026quot;Data from Statistics Canada 2016 Canadian Census - Journey to Work \\n https://www12.statcan.gc.ca/census-recensement/2016/rt-td/jtw-ddt-eng.cfm\u0026quot;)  Map the proportions I\u0026rsquo;m always trying to improve my mapping skills and geom_sf makes it a lot easier.\nontario_commute_clean %\u0026gt;% filter((!live %in% c(\u0026quot;Rainy River\u0026quot;, \u0026quot;Kenora\u0026quot;, \u0026quot;Thunder Bay\u0026quot;, \u0026quot;Algoma\u0026quot;, \u0026quot;Nipissing\u0026quot;, \u0026quot;Cochrane\u0026quot;, \u0026quot;Greater Sudbury / Grand Sudbury\u0026quot;, \u0026quot;Manitoulin\u0026quot;, \u0026quot;Timiskaming\u0026quot;, \u0026quot;Sudbury\u0026quot;, \u0026quot;Parry Sound\u0026quot;))) %\u0026gt;% ggplot() + geom_sf(aes(fill = live_prop)) + scale_fill_viridis_c(\u0026quot;% Labour force who commute to work\u0026quot;, labels = scales::percent) + theme_minimal() + theme(panel.grid = element_blank(), axis.text = element_blank(), axis.ticks = element_blank()) + coord_sf(datum=NA) + labs(title = \u0026quot;The % of the labour force (aged 15+) who commute by Census Division in Southern Ontario\u0026quot;)  ","date":1551571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551636205,"objectID":"9b4daa42aa3daad02e8e964d4106641f","permalink":"/code/analyzing-commuting-patterns-in-ontario/","publishdate":"2019-03-03T00:00:00Z","relpermalink":"/code/analyzing-commuting-patterns-in-ontario/","section":"code","summary":"I was inspired by David Robinson\u0026lsquo;s latest webcast in which he made a heatmap of French train delays using geom_tile and wanted to try it out for myself.\n In this week\u0026#39;s #tidytuesday screencast, I analyze delays in French train stations ðŸ‡«ðŸ‡·ðŸš„","tags":["Open data"],"title":"Analyzing Commuting Patterns in Ontario","type":"code"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":[],"categories":[],"content":"Moral hazard in ALMPs Bruttel, Oliver 2005\n  This research reviews the mechanisms by which government can manage the risks (moral hazard) associated with contracting out of ALMPs using a principal-agent theory lens. the author solves for the risks by advancing an integrated governance approach using 3 distinct governance mechanisms  incentives information control   reviews 3 countries (UK, Netherlands, Australia)  Principal-agent theory and moral hazard  concerned with how does govt (principal) ensure that service providers (agents) act in the govts interest main risk is moral hazard where agents act in their own interest (maximising utility, risk avoidance etc) where principal cannot observe their actions moral hazard in ALMPs revolve around 3 key areas:  creaming - selecting only optimal clients for service parking - not serving clients with low success probability dead-weight - serving clients who would have otherwise been successful without intervention (not explicitly mentioned but described elsewhere)    1. Incentive mechanisms  outcomes-based payments split-fee works best (fixed \u0026amp; outcomes) rather than threshold based (like US) so that award function is linear measured as outputs (outcomes at 13 and 26 weeks are standard) rather than inputs (clients served) should optimize around under-investment i.e. skills training like Job Seeker Account (matching govt investment) common strategy to avoid moral hazard problems is to organize outcomes payments by client groups based on assessment tool (e.g. JSCI in AUS and Kansmeter in Netherlands)  2 . Information mechanisms  benchmarking - measure relative performance of providers  2 types of variables: context (regional/local labour market) and jobseeker characteristics (age, duration of unemployment, skills etc) Australia 5 Star model gold standard - best 60% of providers are offered contract renewal without tender   monitoring - mix of database inputs and in-person  overcome blackbox problem irregularities flagged automatically    3. Control mechanisms  rules and regulations (guidelines, eligibility, etc) quality management frameworks  set minimum standard of service   tied to monitoring and benchmarking  Special expertise  identifies 3 types of capacities governments need  feasibility assessment capacity - make-or-buy decisions about services Implementation capacity (incentive mechanisms) tenders, selecting providers, negotiate contracts etc evaluation capacity (information and control) - evaluate and monitor the performance of providers    Institutional moral hazard in Australia PES F Vandenbroucke, C Luigjes, 2016\n   This research outlines how Austrialia moved from a blackbox approach to contracting out employment services towards a regime of minimum requirements and monitoring\n  reforms of the blackbox approach included:\n new tendering processes (see above) 5 Star rating system for providers adjustment of provider responsibilities standardization of work process    relationship between govt \u0026amp; providers has been rocky as flexibility has been reduced\n  problems were creaming and parking\n  3 streams of service\n stream A - job competitive stream B - vocational issues stream C - series non-vocational issues via referral    Denmark devolved employment services to municipalities\n  Work for the Dole targets long term barriered clients in an attempt to overcome parking (re-vamp of JCP?)\n  the KPIs, outcomes based model also includes a bonus if referred to training\n  outcomes are based on location, stream and expediency (how fast) clients move to outcomes.\n   ","date":1546732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546792979,"objectID":"1ab89e39e252a986f6f75e07d820e734","permalink":"/notes/notes-on-moral-hazard/","publishdate":"2019-01-06T00:00:00Z","relpermalink":"/notes/notes-on-moral-hazard/","section":"notes","summary":"Moral hazard in ALMPs Bruttel, Oliver 2005\n  This research reviews the mechanisms by which government can manage the risks (moral hazard) associated with contracting out of ALMPs using a principal-agent theory lens.","tags":[],"title":"Notes on Moral Hazard","type":"notes"},{"authors":[],"categories":[],"content":"Estimating the Impact of Active Labour Market Programs using Administrative Data and Matching Methods Handouyahia, 2016\n  research link using admin data matched to CRA they find that EBSMs have positive effects on employment and earnings  Findings  EBSMs caused gains in employment earnings, incidence of employment (+2.4% first year to +4.4% by fifth year) and reduction of EI benefits after completing SD had most pronounced effect (between $204 and $4,059 higher) SD represents the largest incremental gains  Labour market program efficacy: Evidence from Ontario Works Adams et. al. 2018\n working paper here and cd howe report here how effective are OW ALMPs on two outcomes:  case duration (benefit \u0026lsquo;spell\u0026rsquo; measured in months) probability of returning to OW within 1 or 2 years of exit   this client group has weaker LM attachment and so not perfectly aligned to existing literature on ALMPs accordingly existing canadian work suggests this group is differentially affected by ALMPs  Findings  placement services extend OW spells by 16 months structured job search not having an effect on \u0026lsquo;return\u0026rsquo; training programs (literacy, skills/vocational) does reduce \u0026lsquo;return\u0026rsquo; by 07% year 1 and 1.1% year 2 among all interventions there\u0026rsquo;s a trade-off between short and long term effectiveness  placement means longer spells but reduces P(return) by 7% in year 1 and 10% year 2 structured job search reduces spell by 1.5 - 3 months but doesn\u0026rsquo;t affect P(return) training program alone is more effective in the short-term than assignment to boht training program and structured job-search (implying the interventions aren\u0026rsquo;t additive)    ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546362854,"objectID":"7f480587cf36eea68fb13601a43cfeec","permalink":"/notes/notes-on-outcomes-in-labour-market-programs/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/notes/notes-on-outcomes-in-labour-market-programs/","section":"notes","summary":"Estimating the Impact of Active Labour Market Programs using Administrative Data and Matching Methods Handouyahia, 2016\n  research link using admin data matched to CRA they find that EBSMs have positive effects on employment and earnings  Findings  EBSMs caused gains in employment earnings, incidence of employment (+2.","tags":[],"title":"Notes on outcomes in labour market programs","type":"notes"},{"authors":[],"categories":[],"content":"  this research evaluates the strictness of rules (eligibility criteria) that govern the receipt of benefits (first-tier/unemployment and second-tier/welfare) across OECD countries they establish what types of eligibility criteria are common across countries they surveyed each OECD country and derived a quantitative policy indicator of eligibility strictness of each scheme they discuss the impacts of the eligibility choices on first and second tier benefits (for Canada, they used Ontario Works) and marginally discuss administrative provisions (how clients interact with administration)  Eligibility Criteria  entitlement conditions: these are the rules set by the feds over which Ontario has little influence (e.g. how many hours of work before claim is established) activity conditions - behavioural requirements that must be met by those who have an established right (\u0026lsquo;entitlement\u0026rsquo;)  availability criteria  availability during ALMP participation (excuse for training/participation) occupational mobility - refuse based on NOC/less pay geography other valid reasons - canada only labour dispute   job-search and reporting criteria  frequency of job-search activity documentation of job-search   sanctions for non-compliance  voluntary resignation / quit refusal of suitable employment repeated refusal of employment refusal of ALMP participation repeated refusal of ALMP participation      First Tier eligibility criteria  job search \u0026amp; reporting criteria vary strongly but availability criteria varies little to none  australia - job search now once a month and Denmark is entirely online Canada among lowest for job-search and reporting requirements    Second Tier criteria  availability criteria represent biggest difference between first and second tier benefit types two types of jurisdictions - 1 where SA clients don\u0026rsquo;t qualify (see Germany) and could be Canada and 2 - where rules are mostly identical - see UK\u0026rsquo;s jobseekers allowance rules are often less strict here to give local authorities more leeway to design appropriate solutions  Insights  warnings and sanctions generally are shown to shorten benefit duration and increase re-employment among those who may expect to incur a sanction (threat effects work) soft constraints like mandatory participation in AMLPs or gradual reductions of benefits levels over time have similar effects to those of harsher instruments like sanctions and dis-entitlements goal is to strengthen incentives to look/prepare for and accept employment and also target those more suitable  Figures ","date":1543104000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543166156,"objectID":"6587b12344bc9f14555629377c6a526d","permalink":"/notes/notes-on-incentives-in-labour-market-programs/","publishdate":"2018-11-25T00:00:00Z","relpermalink":"/notes/notes-on-incentives-in-labour-market-programs/","section":"notes","summary":"this research evaluates the strictness of rules (eligibility criteria) that govern the receipt of benefits (first-tier/unemployment and second-tier/welfare) across OECD countries they establish what types of eligibility criteria are common across countries they surveyed each OECD country and derived a quantitative policy indicator of eligibility strictness of each scheme they discuss the impacts of the eligibility choices on first and second tier benefits (for Canada, they used Ontario Works) and marginally discuss administrative provisions (how clients interact with administration)  Eligibility Criteria  entitlement conditions: these are the rules set by the feds over which Ontario has little influence (e.","tags":[],"title":"Notes on incentives in labour market programs","type":"notes"},{"authors":[],"categories":[],"content":" Algoma Public Health is the public health agency in my area that inspects restaurants to ensure they are abiding by all the relevant food safety legislation. They are also nice enough to publish their restaurant health inspection reports online for anyone to review.\nI searched for a few restaurants that I frequent (phew Fratellis is safe!) but then I wanted to know which restaurants had the most health infractions?\nTheir website isn\u0026rsquo;t setup to answer that kind of question. Each inspection report is published as a unique page on their website which makes aggregating and comparing difficult.\nI used rvest to scrape the URLs of all the reports APH published. Then I scraped the contents of the reports so that I could create a dataset that would allow me to aggregate and compare the results (the dataset and code available here.)\nRestaurants with the most infractions Of the 90% of restaurants who received three visits, here are the top 5 in terms of total number of infractions:\nSurprising results. Upper Deck has some particularly bad health infractions.\nBut that got me thinking\u0026ndash;not all infractions are equal. Some are minor and are corrected onsite. Some are more serious and others still are downright strange. Let\u0026rsquo;s take a closer look.\nNumber of infractions Here are the top 10 infractions by frequency, meaning these infractions appear most often in the dataset.\nStranger Infractions The infractions that don\u0026rsquo;t appear frequently are also quite interesting. Here are infractions that only appear once in the dataset, but leave me with more questions than answers.\n   Restaurant Address Date Infraction     Juicy Beatz \u0026amp; Healthy Eatz 235 McNabb Street 2017-08-23 Food handler hygiene   Pauline\u0026rsquo;s Place 923 Queen Street 2017-11-28 Only Grade A or B eggs permitted   Queen West Variety 602 Queen Street 2016-12-16 Exclusion of live animals on the premises, subject to exemptions   Roberta Bondar Pavilion Serving Kitchens 65 Foster Dr 2016-08-30 No room with food used for sleeping purposes   Teen Challenge Kitchen 1446 Great Northern Road 2016-12-13 Uninspected meats obtained through hunting: only for custom cutting, wash/rinse/sanitize equipment after use as prescribed    Questions that come to mind:\n who\u0026rsquo;s sleeping in the Pavilion? did the Teen Challenge involve catching your meal? what kind of animals are hanging out in Queen West Variety? how does one tell the grade of eggs? Can you do it just by looking?  What does this data tell us? Not too much really :)\nI hope you enjoyed this peek behind the health inspection curtain. The dataset and code are available here. If you have any questions or comments, please feel free to get in touch.\n","date":1512432000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512496925,"objectID":"8aba062471af9a795f67f1a0124ff923","permalink":"/code/scraping-restaurant-inspection-reports/","publishdate":"2017-12-05T00:00:00Z","relpermalink":"/code/scraping-restaurant-inspection-reports/","section":"code","summary":"Algoma Public Health is the public health agency in my area that inspects restaurants to ensure they are abiding by all the relevant food safety legislation. They are also nice enough to publish their restaurant health inspection reports online for anyone to review.","tags":["Open data"],"title":"Scraping Restaurant Inspection Reports","type":"code"},{"authors":["Rob Coleman","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Rob Coleman","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]